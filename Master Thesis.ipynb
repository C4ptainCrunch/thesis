{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mancala\n",
    "\n",
    "[Kalah is solved](http://kalaha.krus.dk/) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Awal√©\n",
    "Also known as awari and owari\n",
    "\n",
    "## Solved\n",
    "\n",
    "It might be solved by [Solving awari with parallel retrograde analysis](http://ieeexplore.ieee.org/abstract/document/1236468/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Game:\n",
    "    def __init__(self, pits=6, seeds=4):\n",
    "        self.pits = pits\n",
    "        self.seeds = seeds\n",
    "        self.reset()\n",
    "        self.actions = np.array(range(self.pits))\n",
    "    \n",
    "    def reset(self):\n",
    "        self.state = np.ones((self.pits * 2), dtype=int) * self.seeds\n",
    "        self.current_player = 0\n",
    "        self.captures = np.zeros((2,), dtype=int)\n",
    "        self.history = set()\n",
    "    \n",
    "    def copy(self):\n",
    "        g = Game()\n",
    "        \n",
    "        g.state = self.state.copy()\n",
    "        g.current_player = self.current_player\n",
    "        g.captures = self.captures.copy()\n",
    "        g.history = self.history.copy()\n",
    "        \n",
    "        return g\n",
    "    \n",
    "    @property\n",
    "    def state_from_current_player(self):\n",
    "        shift = 0 if self.current_player == 0 else self.pits\n",
    "        return np.roll(self.state, shift)\n",
    "\n",
    "    @property\n",
    "    def legal_actions(self):\n",
    "        # todo : add the Let the opponent play rule\n",
    "        return [x for x in range(self.pits) if self.state[x] != 0]\n",
    "\n",
    "\n",
    "    def step(self, action):\n",
    "        assert 0 <= action < self.pits\n",
    "        action = action if self.current_player == 0 else action - self.pits\n",
    "        \n",
    "        seeds = self.state[action]\n",
    "        assert seeds != 0\n",
    "        \n",
    "        # empty the target pit\n",
    "        self.state[action] = 0\n",
    "        \n",
    "        # fill the next pits\n",
    "        pit_to_sow = action\n",
    "        while seeds > 0:\n",
    "            pit_to_sow = (pit_to_sow + 1) % (self.pits * 2)\n",
    "            if pit_to_sow != action: # do not fill the target pit ever\n",
    "                self.state[pit_to_sow] += 1\n",
    "                seeds -= 1\n",
    "        \n",
    "        # capture\n",
    "        # count the captures of the play\n",
    "        captures = 0\n",
    "        if pit_to_sow in self.adverse_pits_idx:\n",
    "            # if the last seed was in a adverse pit\n",
    "            # we can try to collect seeds\n",
    "            while self.state[pit_to_sow] in (2, 3):\n",
    "                # if the pit contains 2 or 3 seeds, we capture them\n",
    "                self.captures[self.current_player] += self.state[pit_to_sow]\n",
    "                captures += self.state[pit_to_sow]\n",
    "                self.state[pit_to_sow] = 0\n",
    "                \n",
    "                # go backwards\n",
    "                pit_to_sow = (pit_to_sow - 1) % (self.pits * 2)\n",
    "                \n",
    "        \n",
    "        # change player\n",
    "        self.current_player = (self.current_player + 1) % 2\n",
    "        \n",
    "        # record state\n",
    "        tstate = tuple(self.state)\n",
    "        if tstate in self.history:\n",
    "            assert False, \"Game loop\"\n",
    "        else:\n",
    "            self.history.add(tstate)\n",
    "        \n",
    "        return captures, self.state, self.game_finished\n",
    "    \n",
    "    @property\n",
    "    def adverse_pits_idx(self):\n",
    "        if self.current_player == 1:\n",
    "            return list(range(self.pits))\n",
    "        else:\n",
    "            return list(range(self.pits, self.pits * 2))\n",
    "    \n",
    "    @property\n",
    "    def game_finished(self):\n",
    "        no_moves_left = np.all(self.state_from_current_player[:self.pits] == np.zeros((self.pits,)))\n",
    "        enough_captures = np.any(self.captures > (self.pits * self.seeds) / 2)\n",
    "        draw = np.all(self.captures == (self.pits * self.seeds) / 2)\n",
    "        loop = tuple(self.state) in self.history\n",
    "        return no_moves_left or enough_captures or draw or loop\n",
    "    \n",
    "    def show_state(self):\n",
    "        if self.game_finished:\n",
    "            print(\"Game finished\")\n",
    "        print(\"Current player: {} - Score: {}/{}\\n{}\".format(\n",
    "            self.current_player,\n",
    "            self.captures[self.current_player],\n",
    "            self.captures[(self.current_player + 1) % 2],\n",
    "            \"-\" * self.pits * 3\n",
    "        ))\n",
    "        \n",
    "        pits = []\n",
    "        for seeds in self.state_from_current_player:\n",
    "            pits.append(\"{:3}\".format(seeds))\n",
    "        \n",
    "        print(\"\".join(reversed(pits[self.pits:])))\n",
    "        print(\"\".join(pits[:self.pits]))\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Shortest game ever recoreded according to [Mancala World](http://mancala.wikia.com/wiki/Oware) : 19 steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = Game()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "\n",
    "g.reset()\n",
    "shortest_game = [(6, 0), (6, 0), (3, 0), (5, 0), (3, 0), (6, 0), (2, 0), (4, 2), (4, 4), (4, 0), (2, 0), (5, 0), (3, 0), (3, 0), (5, 7), (1, 6), (1, 7), (2, 0), (6, 9),]\n",
    "\n",
    "for action, captures in shortest_game:\n",
    "    game_captures, _, _ = g.step(action - 1)\n",
    "    assert captures == game_captures\n",
    "#g.show_state()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tree Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MCTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Node:\n",
    "    def __init__(self, game, parent=None):\n",
    "        self.game = game\n",
    "        self.parent = parent\n",
    "        self.visits = 0\n",
    "        self.wins = 0\n",
    "        self.children = {}\n",
    "    \n",
    "    @property\n",
    "    def is_fully_expanded(self):\n",
    "        return len(self.children) == self.game.pits\n",
    "    \n",
    "    def add_child(game):\n",
    "        return Node(game.copy(), self)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "game = Game()\n",
    "print(game.game_finished)\n",
    "BUDGET = 10\n",
    "\n",
    "tree = Node(g)\n",
    "\n",
    "while not game.game_finished:\n",
    "    for i in range(BUDGET):\n",
    "        \n",
    "        # find a node to expand\n",
    "        node = tree\n",
    "        while node.is_fully_expanded:\n",
    "            node = random.choice(node.children.values())\n",
    "        \n",
    "        # expand node\n",
    "        g = node.game.copy()\n",
    "        while not g.game_finished:\n",
    "            g.step(random.choice(g.legal_actions))\n",
    "            node = node.add_child(game)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MCTS + UCB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep mind\n",
    "\n",
    " * Alpha Go [Mastering the game of Go with deep neural networks and tree search](https://storage.googleapis.com/deepmind-media/alphago/AlphaGoNaturePaper.pdf)\n",
    " * Alpha Go Zero [Mastering the game of Go without human knowledge](https://www.nature.com/articles/nature24270.epdf?author_access_token=VJXbVjaSHxFoctQQ4p2k4tRgN0jAjWel9jnR3ZoTv0PVW4gB86EEpGqTRDtpIz-2rmo8-KG06gqVobU5NSCFeHILHcVFUeMsbvwS-lxjqQGg98faovwjxeTUgZAUMnRQ)\n",
    " * Alpha Zero [Mastering Chess and Shogi by Self-Play with a\n",
    "General Reinforcement Learning Algorithm](https://arxiv.org/pdf/1712.01815.pdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
