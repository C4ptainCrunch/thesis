{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from rules.ipynb\n",
      "Current player: 0 - Score: 0/0\n",
      "------------------\n",
      "  5  5  6  0  5  5\n",
      "  5  5  4  4  4  0\n",
      "importing Jupyter notebook from players.ipynb\n"
     ]
    }
   ],
   "source": [
    "#%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "import itertools\n",
    "import random\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import tqdm\n",
    "\n",
    "import import_ipynb\n",
    "from rules import Game\n",
    "from players import RandomPlayer, GreedyPlayer, MCTSPlayer, UCTPlayer, GreedyUCTPlayer, HumanPlayer\n",
    "\n",
    "np.set_printoptions(edgeitems=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate a dataset\n",
    "\n",
    "We play with our best player (GreedyUCTPlayer) and use the real game states to fill our datasets.\n",
    "\n",
    "If the features are the board and the value to learn is the ratio of wins, we will learn nothing : it depends a lot on the score.\n",
    "\n",
    "If we add the score, we will might then learn that a high score leads to a great ratio of wins, not very interesting. (Or it might not, since the hypothesis \"high score leads to a great ratio of wins\" is the hypothesis of the greedy agent).\n",
    "\n",
    "We will thus try to learn with X as the game state and Y the probabilities of wins for each move.\n",
    "\n",
    "`play_game()` plays a full game between two opponents and return the end state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_game(player, opponent):\n",
    "    game = Game.start_game()\n",
    "    opponent_action = -1\n",
    "\n",
    "    while not game.game_finished:\n",
    "        player_action = player.play(opponent_action)\n",
    "        game, captures, finished = game.step(player_action)\n",
    "\n",
    "        player, opponent = opponent, player\n",
    "        opponent_action = player_action\n",
    "    return game"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`extract_states()` generates a part of the dataset from a full game by extracting the played states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_states(endstate):\n",
    "    X, Y = [], []\n",
    "    \n",
    "    state = endstate\n",
    "    if state.game_finished:\n",
    "        # we skip the last node as it doesn't have any children\n",
    "        state = state.parent\n",
    "    \n",
    "    while state.parent:\n",
    "        X.append(state.view_from_current_player)\n",
    "        Y.append([\n",
    "            (child.wins[state.current_player]/ child.n_playouts) if child and child.n_playouts else 0\n",
    "            for child in state.children\n",
    "        ])\n",
    "        state = state.parent\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Improvement possibility: maybe we should pick the budget in a normal distribution so we can have players with different strenghts compete.\n",
    "We could also use different agents sometimes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUDGET = 50\n",
    "def generate_data(*args):\n",
    "    # Create our players\n",
    "    player = GreedyUCTPlayer(0, BUDGET)\n",
    "    opponent = GreedyUCTPlayer(1, BUDGET)\n",
    "    \n",
    "    # Run a full game\n",
    "    play_game(player, opponent)\n",
    "    \n",
    "    # Extract states\n",
    "    X1, Y1 = extract_states(player.root)\n",
    "    X2, Y2 =extract_states(opponent.root)\n",
    "    \n",
    "    return X1 + X2, Y1 + Y2\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fd93440da2b4608b7d912743539efbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/html": [
       "<p>Failed to display Jupyter Widget of type <code>HBox</code>.</p>\n",
       "<p>\n",
       "  If you're reading this message in the Jupyter Notebook or JupyterLab Notebook, it may mean\n",
       "  that the widgets JavaScript is still loading. If this message persists, it\n",
       "  likely means that the widgets JavaScript library is either not installed or\n",
       "  not enabled. See the <a href=\"https://ipywidgets.readthedocs.io/en/stable/user_install.html\">Jupyter\n",
       "  Widgets Documentation</a> for setup instructions.\n",
       "</p>\n",
       "<p>\n",
       "  If you're reading this message in another frontend (for example, a static\n",
       "  rendering on GitHub or <a href=\"https://nbviewer.jupyter.org/\">NBViewer</a>),\n",
       "  it may mean that your frontend doesn't currently support widgets.\n",
       "</p>\n"
      ],
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1440), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import multiprocessing\n",
    "\n",
    "pool = multiprocessing.Pool(4)\n",
    "generator = list(range(4*180*2))\n",
    "data = list(tqdm.tqdm_notebook(pool.imap(generate_data, generator), total=len(generator)))\n",
    "\n",
    "pool.close()\n",
    "pool.join()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(list(itertools.chain(*[x for x, y in data])))\n",
    "Y = np.array(list(itertools.chain(*[y for x, y in data])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have $N\\approx 200000$ points to do our learning, the features are a vector of 12 and the target 6 floats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((193598, 12), (193598, 6))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, Y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"data/X.npy\", 'wb') as fd:\n",
    "    np.save(fd, X)\n",
    "\n",
    "with open(\"data/Y.npy\", 'wb') as fd:\n",
    "    np.save(fd, Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Y_norm` is the target normalized. This might help us later if the output of our learning algorithm are probabilities summing to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sums = np.sum(Y, axis=1, keepdims=1)\n",
    "sums[sums==0] = 1\n",
    "Y_norm = Y / sums"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We store in `Yb` a vector (N, 1) : we only keep the index of the best move"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "Yb = np.argmax(Y, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We check is the problem is balanced or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.title(\"Distribution of the actions\")\n",
    "plt.hist(Yb, bins=range(7));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is slightly unbalanced but it should not pose a real problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Test split\n",
    "We have data, lets cut that in a test and train set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test, Yb_train, Yb_test = train_test_split(X, Y, Yb, test_size=0.33)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we test a simple Descision tree with a classification problem : what is the best action to play given a state ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "tree = DecisionTreeClassifier()\n",
    "tree.fit(X_train, Yb_train);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then score the model. The default scoring metric of sklean for this model is the subset accuracy : \"how many times the model predicted the correct class ?\". A value of 1 is the best we can achieve, 1/6 is random and 0 is the worst case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5125219133483596"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tree.score(X_test, Yb_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This seems rather good as the random behaviour is only $1/6$ not $\\approx 1/2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TreeClassificationPlayer(UCTPlayer):    \n",
    "    def default_policy(self, node):\n",
    "        legal_actions = node.legal_actions\n",
    "        assert len(legal_actions) != 0\n",
    "        \n",
    "        # Ask the predictor the best move to play\n",
    "        prediction = tree.predict(node.view_from_current_player.reshape(1, -1))[0]\n",
    "        \n",
    "        # 10% of the time we choose at random and to not follow the classifier\n",
    "        if random.random() < 0.1 or prediction not in legal_actions:\n",
    "            return random.choice(legal_actions)\n",
    "        \n",
    "        return prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now compare our two agents : UCT and the model above. Both with a budget of 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(*args):\n",
    "    # Create our players\n",
    "    player = TreeClassificationPlayer(0, 50)\n",
    "    opponent = UCTPlayer(1, 50)\n",
    "    \n",
    "    # Run a full game\n",
    "    game = play_game(player, opponent)\n",
    "    \n",
    "    return game.winner\n",
    "\n",
    "import multiprocessing\n",
    "\n",
    "pool = multiprocessing.Pool(4)\n",
    "generator = list(range(20))\n",
    "\n",
    "data = list(tqdm.tqdm_notebook(pool.imap(test, generator), total=len(generator)))\n",
    "wins = sum(data) / len(data)\n",
    "losses = 1 - wins\n",
    "\n",
    "plt.bar(0, losses)\n",
    "plt.bar(1, wins)\n",
    "plt.xticks([0, 1], [\"Enhanced\", \"UCT\"]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The enhacned model is signinficantly better (with p=95%)\n",
    "\n",
    "The tree classifier has another output : the probability of the point being each class.\n",
    "We can use this ability to weigh each action when selecting at random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TreeClassificationProbabilityPlayer(UCTPlayer):    \n",
    "    def default_policy(self, node):\n",
    "        legal_actions = node.legal_actions\n",
    "        assert len(legal_actions) != 0\n",
    "        \n",
    "        # Ask the predictor for class probabilities\n",
    "        predictions = tree.predict_proba(node.view_from_current_player.reshape(1, -1))[0]\n",
    "        \n",
    "        # Set probability to 0 for the non legal actions\n",
    "        \n",
    "        weights = [\n",
    "            p + 0.1 if i in legal_actions else 0\n",
    "            for i, p in enumerate(predictions)\n",
    "        ]\n",
    "        \n",
    "        return random.choices(range(6), weights=weights)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(*args):\n",
    "    # Create our players\n",
    "    player = TreeClassificationProbabilityPlayer(0, 50)\n",
    "    opponent = UCTPlayer(1, 50)\n",
    "    \n",
    "    # Run a full game\n",
    "    game = play_game(player, opponent)\n",
    "    \n",
    "    return game.winner\n",
    "\n",
    "import multiprocessing\n",
    "\n",
    "pool = multiprocessing.Pool(4)\n",
    "generator = list(range(20))\n",
    "\n",
    "data = list(tqdm.tqdm_notebook(pool.imap(test, generator), total=len(generator)))\n",
    "wins = sum(data) / len(data)\n",
    "losses = 1 - wins\n",
    "\n",
    "plt.bar(0, losses)\n",
    "plt.bar(1, wins)\n",
    "plt.xticks([0, 1], [\"Enhanced w/ probabilities\", \"UCT\"]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(*args):\n",
    "    # Create our players\n",
    "    player = TreeClassificationPlayer(0, 50)\n",
    "    opponent = TreeClassificationProbabilityPlayer(1, 50)\n",
    "    \n",
    "    # Run a full game\n",
    "    game = play_game(player, opponent)\n",
    "    \n",
    "    return game.winner\n",
    "\n",
    "import multiprocessing\n",
    "\n",
    "pool = multiprocessing.Pool(4)\n",
    "generator = list(range(20))\n",
    "\n",
    "data = list(tqdm.tqdm_notebook(pool.imap(test, generator), total=len(generator)))\n",
    "wins = sum(data) / len(data)\n",
    "losses = 1 - wins\n",
    "\n",
    "plt.bar(0, losses)\n",
    "plt.bar(1, wins)\n",
    "plt.xticks([0, 1], [\"Tree\", \"Tree w/ prob.\"]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(*args):\n",
    "    # Create our players\n",
    "    player = TreeClassificationPlayer(0, 50)\n",
    "    opponent = GreedyUCTPlayer(1, 50)\n",
    "    \n",
    "    # Run a full game\n",
    "    game = play_game(player, opponent)\n",
    "    \n",
    "    return game.winner\n",
    "\n",
    "import multiprocessing\n",
    "\n",
    "pool = multiprocessing.Pool(4)\n",
    "generator = list(range(20))\n",
    "\n",
    "data = list(tqdm.tqdm_notebook(pool.imap(test, generator), total=len(generator)))\n",
    "wins = sum(data) / len(data)\n",
    "losses = 1 - wins\n",
    "\n",
    "plt.bar(0, losses)\n",
    "plt.bar(1, wins)\n",
    "plt.xticks([0, 1], [\"Tree\", \"Greedy UCT\"]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree.feature_importances_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
